<!DOCTYPE html>
<html>
<head>
<title>APT Documentation</title>
<link rel="stylesheet" type="text/css" charset="utf-8" media="all" 
href="styles/common.css">
<link rel="stylesheet" type="text/css" charset="utf-8" media="screen" 
href="styles/screen.css">
<link rel="stylesheet" type="text/css" charset="utf-8" media="print" 
href="styles/print.css">
<link rel="stylesheet" type="text/css" charset="utf-8" 
media="projection" href="styles/projection.css">

<style type="text/css">
strong.regular-font {
  font-family: Arial, Lucida Grande, sans-serif;
  font-style: italic;
  font-size: 0.9em;
}
</style>

</head>

<body>
<h1><a href="index.html">APT</a> - The Animal Part Tracker</h1>

<br>
<br>
<center><img style='border:2px solid #000000' src="images/apt_examples.png" width="50%"></center>
<br/>

<p>The <b>Animal Part Tracker</b> (<b>APT</b>) is machine learning software for automatically tracking the locations of body parts in input videos. For example, it has been used to track the tips of the legs of a fly or the ears of a mouse. To train a tracker, a user labels the locations of selected landmark points in selected frames and videos. Machine learning is then used to train a classifier which can automatically predict the locations of these parts in new frames and videos.</p>

<p>APT is modular software that has been engineered to work efficiently for a variety of applications. It has been used to track parts of flies, mice, and larvae. It can track parts in two dimensions from video from a single camera, or in three dimensions from video from multiple calibrated cameras. It can also be used to track the parts of multiple interacting animals. APT includes both an engineered labeling interface as well as machine learning algorithms for training trackers.</p>

<p>APT is developed by Allen Lee, Mayank Kabra, Alice Robie, Stephen Huston, Felipe Rodriguez, Roian Egnor, Austin Edwards, and Kristin Branson. All work is funded by the Howard Hughes Medical Institute and the Janelia Research Campus.</p>

<p>This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the <a href="GNU_GPL_v3.html">GNU General Public License (version 3)</a> for more details.</p>

<hr class="h2-divider">

<h2>Links</h2>
<ul>
<li> <b>Git repository</b> with the latest version of the code:
<a href="https://github.com/kristinbranson/APT">https://github.com/kristinbranson/APT</a></li>
<li> <b>Branson lab</b> at HHMI Janelia Research Campus: <a href="http://www.janelia.org/lab/branson-lab">http://www.janelia.org/lab/branson-lab</a></li>
<li> <b>Contact information</b>: Kristin Branson bransonk@janelia.hhmi.org</li>
</ul>

<hr class="h2-divider">

<h2>Contents</h2>

<ul>
<li><a href="#Download">Download</a></li>
<li><a href="#Requirements">Requirements</a></li>
<!--
<li><a href="#Install">Installation</a></li>
-->
<li><a href="#Startup">Start-up</a></li>
<li><a href="#Setting up a new project">Setting up a new project</a></li>
<li><a href="#Opening an existing project">Opening an existing project</a></li>
<li><a href="#Navigating in APT">Navigating in APT</a></li>
<li><a href="#Labeling points">Labeling points</a></li>
<li><a href="#Setting the tracking parameters">Setting the tracking parameters</a></li>
<li><a href="#Training">Training</a></li>
<li><a href="#Tracking">Tracking</a></li>
<li><a href="#Exporting tracking results">Exporting tracking results</a></li>
<li><a href="#Exporting manual labels">Exporting manual labels</a></li>
<li><a href="#Cross validation">Cross validation</a></li>
<li><a href="#Ground-Truthing mode">Ground-Truthing mode</a></li>
<li><a href="appendices.html">Appendices</a></li>

</ul>

<hr class="h2-divider">

<h2><a id="Download">Download</a></h2>

<p>The <a href="https://github.com/kristinbranson/APT">git repository</a> for APT is hosted on <a href="https://github.com/kristinbranson/APT"><b>Github</b></a>. This software is currently under heavy development, so we recommend using git to <a href="https://help.github.com/en/articles/cloning-a-repository">clone</a> the repository and updating frequently.</p>

<!--
<p>APT requires the following other code packs; please also download these:
<ul>
<li> <a href="https://github.com/kristinbranson/JAABA">JAABA</a>: The Janelia Automatic Animal Behavior Annotator.</li>
<li> <a href="https://github.com/pdollar/toolbox">Piotr Dollar's Computer Vision Toolbox</a>.
</ul>
</p>
-->

<!-- <p>If you are working with multiple calibrated cameras, you may need calibration software for your rig, e.g <a href="https://www.vision.caltech.edu/bouguetj/calib_doc/"> Caltech camera calibration toolbox </a>
-->

<hr class="h2-divider"/>

<h2><a id="Requirements">Requirements</a></h2>

<h3>MATLAB</h3>

<p>APT requires <a href="https://www.mathworks.com/products/matlab.html">MATLAB</a> version >= 2017a and the following MATLAB toolboxes:
<ul>
<li>Image Processing Toolbox</li>
<li>Parallel Computing Toolbox</li>
<li>Statistics and Machine Learning Toolbox</li>
<li>Curve Fitting Toolbox (<i>Multi-camera only</i>)</li>
<li>Computer Vision System Toolbox (<i>Multi-camera only</i>)</li>
<li>Optimization Toolbox (<i>Multi-camera only</i>)</li>
</ul>
</p>

<h3>GPU</h3>

<p>A <a href="https://developer.nvidia.com/cuda-gpus">CUDA-enabled</a> <a href="https://lambdalabs.com/blog/best-gpu-tensorflow-2080-ti-vs-v100-vs-titan-v-vs-1080-ti-benchmark">GPU</a> is required to train a deep-learning based tracker. This GPU can be:
<ul>
<li>On your local machine.</li>
<li>On the Amazon Cloud (AWS).</li>
<li>On the Janelia GPU cluster (Janelians-only).</li>
</ul>
We refer to this as the <a href="#BackEnd"><b>back end</b></a>, and more information about setting up each of these back ends is <a href="#BackEnd">here</a>.</p>

<p>If you choose to use a <b>local back end</b>, your local machine must have an NVIDIA GPU with <a href="https://en.wikipedia.org/wiki/CUDA#GPUs_supported">Compute Capability >= 3.0</a>. The best GPUs for deep learning are constantly changing (some benchmark info is <a href="https://lambdalabs.com/blog/tag/benchmarks/">here</a>). It is important for this GPU to have sufficient memory. All GPUs we have used have at least 12GB of RAM. In our lab, we have used the following GPUs:
<ul>
<li>V100</li>
<li>2080 Ti</li>
<li>Titan Xp (older)</li>
<li>Titan X (older)</li>
<li>1080 Ti (older)</li>
</ul>
</p>

<hr class="h2-divider">

<h2><a id="BackEnd">Setting up GPU back ends for deep learning</a></h2>

<p>The <b>GPU back end</b> is where and how GPU-based deep-learning computations are performed. Which back end to choose depends on your resources:
<ul>
<li>If you are at Janelia, we recommend using the <b><a href="#JaneliaBackEnd">Janelia GPU cluster</a></b> back end.</li>
<li>If you have one or many CUDA-enabled GPUs in your local machine, we recommend using the <b><a href="#LocalBackEnd">local</a></b> back end. </li>
<li>Otherwise, we reccommend using the <b><a href="#AWSBackEnd">AWS</a></b> back end to train on the Amazon Cloud.</li>
</ul>
Instructions for setting up each of these back ends follows. </p>

<h3><a id="LocalBackEnd">Local GPU back end</a></h3>

We use <b><a href="#CondaLocalBackEnd">Conda</a></b> to install dependencies on Windows OS and <b><a href="#DockerLocalBackEnd">Docker</a></b> containers to provide dependencies on Linux.

<h4><a id="DockerLocalBackEnd">Docker setup - Linux</a></h4>

Requirements:
<ul>
  <li> <a href="https://www.nvidia.com/Download/index.aspx">CUDA version >= 418.39</a></li>
  <li> <a href="https://docs.docker.com/install/">Docker</a></li> 
  <li> <a href="https://github.com/NVIDIA/nvidia-docker">nvidia-docker</a></li>
</ul>
The following instructions will take you, step-by-step, through installing and configuring these requirements. 
<ul>
  <li>
    Install <b>CUDA</b> <a href="https://docs.nvidia.com/deploy/cuda-compatibility/index.html#binary-compatibility__table-toolkit-driver">version >= 418.39</a>. This can be done by downloading and installing either the
    <ul>
      <li><a href="https://developer.nvidia.com/cuda-downloads">CUDA Toolkit</a> or</li>
      <li><a href="https://www.nvidia.com/Download/index.aspx">CUDA driver</a> only.</li>
    </ul>
  </li>
  <li>
    Install <b><a href="https://docs.docker.com/install/">Docker</a></b> following the <a href="https://docs.docker.com/install/">instructions on Docker's website</a>. The free Community Edition is fine. 
  </li>
  <li>
    Set up Docker so that it <b>does not require sudo</b> privileges as described here: <a href="https://docs.docker.com/install/linux/linux-postinstall/">Manage Docker as a non-root user</a>.
  </li>
  <li>
    Install <b><a href="https://github.com/NVIDIA/nvidia-docker">nvidia-docker</a></b>.
  </li>
  <li>
    Download the latest pre-built APT docker container by running the following in a Linux terminal:
    <pre>docker pull bransonlabapt/apt_docker:latest</pre>
  </li>
  <li>Test. Here are some test commands you can try to check that everything is working as expected:
    <ul>
      <li>
	Test your Docker installation:
	<pre>docker run hello-world</pre>
      </li>
      <li>
	Test your nvidia-docker installation:
	<pre>docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi</pre>
      </li>
    </ul>
  </li>
</ul>

To use this back end within APT, select <b>Local (Docker)</b> under the <b>Track > GPU/Backend Configuration</b> menu. You can test that everything hooks up between Docker and APT by selecting menu item <b>Track > GPU/Backend Configuration > Test backend configuration</b>. 

<h4><a id="CondaLocalBackEnd">Conda setup - Windows</a></h4>

Requirements:
<ul>
  <li> <https://www.anaconda.com/distribution/>Anaconda Python 3.7</a></li>
  <li> <a href="https://www.nvidia.com/Download/index.aspx">CUDA version >= 418.39</a></li>
<li>Python modules installed with Conda: tensorflow-gpu=1.13.1, keras=2.2.4, opencv=3.4.4 hdf5storage=0.1.15 imageio=2.5.0 scikit-image=0.15.0 matplotlib=3.0.3 future=0.17.1 easydict=1.9</li>
</ul>
The following instructions will take you, step-by-step, through installing and configuring these requirements. 
<ul>
  <li>
    Install <https://www.anaconda.com/distribution/><b>Anaconda Python 3.7</b> version for Windows</a>.
</li>
  <li>
    Install <b>CUDA</b> <a href="https://docs.nvidia.com/deploy/cuda-compatibility/index.html#binary-compatibility__table-toolkit-driver">version >= 418.39</a>. This can be done by downloading and installing either the
    <ul>
      <li><a href="https://developer.nvidia.com/cuda-downloads">CUDA Toolkit</a> or</li>
      <li><a href="https://www.nvidia.com/Download/index.aspx">CUDA driver</a> only.</li>
    </ul>
  <li>
    The following is done at an Anaconda prompt (Start Menu > Anaconda3 > Anaconda Prompt):
    <ul>
      <li>
	<b>Update Conda</b>:
	<pre>conda update conda
conda upgrade --all</pre>
      </li>
    <li>
    Create and activate the <b>APT environment</b>. 
    <pre>conda create --name APT
conda activate APT</pre>
    </li>
    <li><b>Install dependencies</b>.
    <pre>conda install -c anaconda tensorflow-gpu=1.13.1 keras=2.2.4
conda install -c conda-forge opencv=3.4.4 hdf5storage=0.1.15 imageio=2.5.0 scikit-image=0.15.0 matplotlib=3.0.3 future=0.17.1 easydict=1.9</pre>
    </li>
    </ul>
</ul>

To use this back end within APT, select <b>Local (Conda)</b> under the <b>Track > GPU/Backend Configuration</b> menu. You can test that everything hooks up between Conda and APT by selecting menu item <b>Track > GPU/Backend Configuration > Test backend configuration</b>. 

<h3><a id="AWSBackEnd">AWS GPU back end</a></h3>

The steps below will guide you through setting up and configuring <a href="https://aws.amazon.com/ec2/">Amazon Web Services Elastic Cloud Compute (AWS EC2)</a> for use with APT. After you are done with the setup, APT will be able to perform deep learning training and inference in the cloud.

<h4>About EC2 instances</h4>

You can think of an <b>EC2 instance</b> (as configured below) as a computer with a GPU in the cloud that is under your control. It is your "GPU in the cloud". An instance has:
<ul>
<li>An <b>instance ID</b>, which uniquely identifies it.</li>
<li>A (public) <b>IP address</b> so you can connect to it via <a href="https://en.wikipedia.org/wiki/Secure_Shell">ssh</a>, examine its processes and filesystem contents. </li>
<li>A <b>state</b> that is either <code>RUNNING</code>, <code>STOPPED</code>, or <code>TERMINATED</code>:
  <ul>
    <li>If the state is <code>RUNNING</code>, your instance is either actively computing or ready to do so. You are paying about $1/hour.</li>
    <li>A state of <code>STOPPED</code> is analogous to having your desktop workstation in "hibernation" or "sleep" mode. No computations are being carried out, but any previous computations (trained models) etc are saved in the remote filesystem. You are paying a very low price based on the amount of disk storage ("EBS" storage in AWS-speak) that has been allocated for your instance. For 50GB of storage (the current default), you are paying $5/month.</li>
    <li>A state of <code>TERMINATED</code> means your that instance has ceased to exist and returned to the ephemeral protoplasm of the cloud! Any and all state is destroyed at termination, so before you terminate an instance, make sure you tell APT to download all your trained models. As you might guess, terminated instances do not cost anything.</li>
  </ul>
</ul>

<p>Since <code>STOPPED</code> instances are inexpensive, APT is currently designed around the idea that you will create an EC2 instance and "leave it up" for a stretch of time (eg weeks, maybe even a month or two) while you do a bunch of work for a project. During this time, you can iteratively label, train, and track within APT over multiple sessions. Between active working sessions, your instance is <code>STOPPED</code>, and all APT state including trained models and movies/trxfiles to be tracked is preserved. After a time, you will reach a stopping point for the project, and you can instruct APT to download your trained models to your local workstation. (Tracking results are currently downloaded immediately after each tracking session.) When the download is complete, you can terminate the EC2 instance.</p>

<p>APT automates starting/stopping your instance (TODO: check), starting/stopping training and tracking processes, and so on. However, it is inevitable that APT will at times become disconnected from what is happening in the cloud. At these times, manually managing the EC2 instance by eg ssh-ing into the instance and killing processes, or manually stopping an instance via the AWS dashboard will be necessary. More information on how to do this is <a href="TODO">here</a>.</p>

<p>Be proactive and check on your instance!</p>

<h4>EC2 Setup</h4>

<ul>
  <li><b>Create an AWS account</b> and set up the payment system. This is the <b>root account</b> for AWS. You can create an account <a href="https://aws.amazon.com/">here</a>.</li>
  <li><b><a href="https://docs.aws.amazon.com/IAM/latest/UserGuide/id_users_create.html#id_users_create_console">Create an AWS user</a></b> (or users) from within the root account:
    <ul>
      <li>Choose a <b>user name</b> and set the <b>Access type</b> to <b>Programmatic Access</b>.
	<center><a href="aws_setup_screenshots/Add%20user%201%20Screenshot%20from%202018-09-25%2013_45_06.png"><img width=50% src="aws_setup_screenshots/Add%20user%201%20Screenshot%20from%202018-09-25%2013_45_06.png"/></a></center>
      </li>
      <li>To set permissions, select <b>Attach existing policies directly</b>, and add <b>Administrator Access</b> and <b>IAMUserChangePassword</b> permissions.
	<center><a href="aws_setup_screenshots/Add%20user%203%20Screenshot%20from%202018-09-25%2013_49_25.png"><img width=50% src="aws_setup_screenshots/Add%20user%203%20Screenshot%20from%202018-09-25%2013_49_25.png"/></a></center>
      </li>
      <li>Login to the AWS user account using the console.</li>
      <li><b>Create Access Keys</b>: 
	<ul>
	  <li>To create a new Access Key pair for an IAM user, open the <a href="https://console.aws.amazon.com/iam">IAM console</a> or look for IAM under <b>Services - Security, Identity & Compliance</b> on your console home page.</li>
	  <li>Click <b>Users</b> in the <b>Details</b> pane, click the appropriate IAM user, and then click <b>Create Access Key</b> on the <b>Security Credentials</b> tab. 
	    <center><a href="aws_setup_screenshots/Access%20Keys%202%20Screenshot%20from%202018-09-25%2013_54_35.png"><img width=50% src="aws_setup_screenshots/Access%20Keys%202%20Screenshot%20from%202018-09-25%2013_54_35.png"/></a></center></li>
	  <li>Save the access keys anywhere (I save it in my .ssh folder). 
<center><a href="aws_setup_screenshots/Access%20Keys%203%20Screenshot%20from%202018-09-25%2013_57_07.png"><img width=50% src="aws_setup_screenshots/Access%20Keys%203%20Screenshot%20from%202018-09-25%2013_57_07.png"/></a></center></li>
![](https://github.com/kristinbranson/APT/blob/feature/deeptrack/docs/aws_setup_screenshots/Access%20Keys%203%20Screenshot%20from%202018-09-25%2013_57_07.png)
Access keys are sort of pair of login and password that are required to start an instance.
  * On Linux, change the permission to read only using “chmod 400 path/to/access_key.csv”
  * On Windows, ...

* Create an ssh Key Pair. For this, go to the EC2 console (https://console.aws.amazon.com/ec2) or look for EC2 under Services - Compute on your console home page). On the left, click on **Key Pairs** under **Network & Security** and then use the **Create Key Pair** button. Save the key pair in your ~/.ssh folder and change the permissions to read only using “chmod 400 ~/.ssh/key_pair.pem”. This ssh Key Pair is different than the above Access Keys. The ssh Key Pair is used to ssh into a running instance while Access Keys are required to create an instance. Alternatively, if you already have a key pair, you can import your public key by clicking **Import Key Pair**. 

* Increase the limit of the number of instances you can create of type p2.xlarge. p2.xlarge is the basic instance type that has the GPU useful for APT backend computations. Its cost is 0.9$/hr. Go to the EC2 console (https://console.aws.amazon.com/ec2), use the **Limits** option on the left and request a limit increase for p2.xlarge. My suggestion is to increase the limit to at least 2 instances, so that you can run stuff on one instance and test on the other instance. This step usually takes a day or so because Amazon does verifications. You can, however, continue with the rest of the set up. 

* Install the AWS CLI (https://docs.aws.amazon.com/cli/latest/userguide/installing.html). 

* Configure the AWS CLI (https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html#cli-quick-configuration). This is where you’ll need to enter the Access Keys. (For JRC users, the default region is us-east-1, Northern Virginia). Chose the default output format as json.
![](https://github.com/kristinbranson/APT/raw/feature/deeptrack/docs/aws_setup_screenshots/Configure%20AWS%20CLI%20Screenshot%20from%202018-09-25%2014_40_17.png)

* Check your CLI setup using “aws ec2 describe-regions --output table”. The output should be:
```
----------------------------------------------------------
|                     DescribeRegions                    |
+--------------------------------------------------------+
||                        Regions                       ||
|+-----------------------------------+------------------+|
||             Endpoint              |   RegionName     ||
|+-----------------------------------+------------------+|
||  ec2.ap-south-1.amazonaws.com     |  ap-south-1      ||
||  ec2.eu-west-3.amazonaws.com      |  eu-west-3       ||
||  ec2.eu-west-2.amazonaws.com      |  eu-west-2       ||
```

* Create a security group: The security group defines the basic firewall for the instance that will be launched using the CLI. Below we create a security group “apt_dl” which allows instances to accept ssh connections from any IP address. 
**Important:** Currently you must name your security group "apt_dl" as APT will expect this naming!
```
$ aws ec2 create-security-group --group-name apt_dl --description "Basic security group for APT deep learning"
{
    "GroupId": "sg-b018ced5"
}
$ aws ec2 authorize-security-group-ingress --group-name apt_dl --protocol tcp --port 22 --cidr 0.0.0.0/0
```
You do not need to do this if another user on your account has already created this security group. 

* Once the limits on the number of instances is increased, check that you can launch and ssh into an instance using the EC2 console. 

Use the **Launch Instance** button on your EC2 console page (https://console.aws.amazon.com/ec2). 

Select the AMI **AMI-APT (ami-0168f57fb900185e1)** that we created. You can find this by searching for **0168f57fb900185e1** within **Community AMIs**. 
![](https://github.com/kristinbranson/APT/raw/feature/deeptrack/docs/aws_setup_screenshots/Find%20AMI%20Screenshot%20from%202018-09-25%2017_29_13.png)

Next, select **p2.xlarge** as the instance type. 
![](https://github.com/kristinbranson/APT/raw/feature/deeptrack/docs/aws_setup_screenshots/AMI%20Instance%20Type%20Screenshot%20from%202018-09-25%2016_39_11.png)

Then click **Review and launch**. It will then prompt you for a key pair. Select the ssh key pair you created or uploaded previously. 
![](https://github.com/kristinbranson/APT/raw/feature/deeptrack/docs/aws_setup_screenshots/Launch%20Status%20Screenshot%20from%202018-09-25%2016_42_57.png)

After launching, go to the EC2 console and use the Instances option to see the information about your instances. If you select the recently launched instance in the table, you’ll see all the information regarding this instance in particular its IP address (**IPv4 Public IP**). Use the IP address to ssh into the machine using “ssh -i ~/.ssh/<key_pair.pem> ubuntu@<IPaddress>”. Replace <key_pair.pem> with the ssh key pair you created and downloaded previously. If instead you imported an existing key into Amazon, you can replace this with your private key file. 

* Once you are done with ssh and ready to kill the instance, use the Actions button at the top of the instances page, to Terminate the machine from the Instance State menu. 

## Other APT System Requirements

Depending on your platform, APT also requires some utilities in order to communicate with EC2. 

* Windows
  * ssh and scp. A version of these utilities is distributed with a typical Windows Git installation at `c:\Program Files\Git\usr\bin\ssh.exe` and `...\scp.exe`. Currently these paths are **hardcoded** so these utilities must live precisely in those locations! 
  * (future) For ssh and scp, just go with what's on the PATH.
  * (future) maybe support rsync 
  * The utility `certUtil.exe` is used to compute hashes over files. This should be present in standard Windows installs. To confirm, open a cmd window (terminal) in Windows and type `where certUtil`.





<h3><a id="JaneliaBackEnd">Janelia Cluster GPU back end</a></h3>



<!--
<hr class="h2-divider">

<h2><a id="Install">Installation</a></h2>

In the APT directory, copy Manifest.sample.txt  to Manifest.txt and edit to point to your copy of JAABA and Piotr's toolbox (specify the root directory, which contains the subfolders filehandling/ and misc/, 
e.g. jaaba, c:\pgms\jaaba
	piotr, c:\pgms\pdollar_toolbox).
 
NOTE: In Manifest.sample.txt there is also a path to camera/calibration/toolbox, if you are not using multi-camera it is OK to not include this line.
-->


<hr class="h2-divider">

<h2><a id="Startup">Start-up</a></h2>

APT is a MATLAB-based program. To run it:
<ol>
<li> Start MATLAB. </li>
<li> Within MATLAB, change into the directory containing the APT code:
<pre>>> cd APT;</pre>
</li>

<li> Run <b>StartAPT</b> on the command line to start APT:
<pre>>> StartAPT; 
</pre>
</li>
The <a href="images/apt_open_screen.png" target="_blank">main APT GUI</a> will then appear. 
</ol>

<hr class="h2-divider">

<h2><a id="BackEnd">GPU Back ends</a></h2>

To train and track using deep learning, APT can either use GPUs on your local machine, or remote GPUs in the Amazon Cloud (AWS) or on Janelia's GPU cluster (Janelians-only). 

<hr class="h2-divider">

<h2><a id="Setting up a new project"> Setting up a new project </a></h2>
From the File dropdown menu, select "New Project...".  This will pop up a <a href="images/apt_new_project_dialog.png" target="_blank"> New Project dialog. </a>  Enter a project name, the number of body points to detect, the number of views, and check the "body tracking" box if your animal(s) are tracked. Click "Create Project".  This will open a <a href="images/apt_new_project_add_movies.png" target="_blank">Manage Movies dialog.</a>  Click "Add Movie" and select the first movie to add.  If you selected body tracking, you will also be prompted to select the tracking file associated with this movie.  You may add additional movies at any time.  The Manage Movies dialog box can be used to add more movies and also navigate between movies during labeling. Close or minimize the Manage Movie window. Select "Save as ..." from the File dropdown menu and save your project. This will save a .lbl file with your project in it.

<hr class="h2-divider">

<h2><a id="Opening an existing project"> Opening an existing project </a></h2>

Use "Load Project" from the File dropdown menu and select the .lbl file to load. If the locations of the movies in the project has changed since you last opened the project it will prompt for their new locations.  


<hr class="h2-divider">

<h2><a id="Navigating in APT"> Navigating in APT </a></h2>

The  <a href="images/apt_gui_layout_develop_labeled.png" target="_blank"> <b> GUI is divided into several sections.</b> </a>  Hovering over different components will reveal tooltips.

To navigate through the video in the Labeling window:
a/d, -/=, or the left and right arrow keys all move one frame left/right respectively. Using these keys while holding down the ctrl button uses 10 frames steps (this default step size can be changed in the Go dropdown menu, under "Navigation preferences..."). You can also navigate using the mouse and the navigation bar under the image.  Clicking on the bar to the left or right of the slider jumps by 100 frames.  Single clicks on the arrows at the ends of the navigation bar moves by a single frame.  Holding the end arrows down steps through the movie a frame at a time. To look at only labeled frames, hold the shift button while using the right and left arrow keys.
<br>
<br>
Useful view options for multi-target projects with tracking and orientation:<br>
<i>From the View dropdown menu:</i><br>
Center video on target  - keeps current target in the center of the view<br>
Rotate video so target always points up - does just that<br>
Zoom out/full images or unzoom button - view full image <br>
<br>
In the Target table window: <br>
Use slider bar above ‘unzoom’ to adjust zoom, not the magnifier +/- <br>
Zoom slider bar - adjust zoom then ‘set’, ‘recall’ will change back to ‘set’ value after changing movies. 

<p>To just look at labeled frames, holding the shift button while using the right and left arrow keys will move through only the labeled frames, skipping the unlabeled ones. </p>


<hr class="h2-divider">

<h2><a id="Labeling points"> Labeling points </a></h2>

Labeling modes:
There are different labeling modes, which you can switch between in the Label dropdown menu.

<ul>
<li><b>Sequential mode:</b><br>
Allows you to just click your points in order; points are automatically accepted once you click the last point.  You can edit placement after you finish by:
<ul>
<li>Clicking and dragging a point</li>
<li>Activating a point with a number key that corresponds to the label number and then clicking the correct location for the label. Activated points change from '+' to 'x' (Numbers above 10 are accessed by `(back quote) + number key).</li>
<li>Activating a point with the number keys and moving with arrow keys. Hitting the number key again will toggle the point back to '+'. If you hold down the shift key while you click, the point will be recorded as an 'o' instead of a '+', indicating an occluded point.</li> 
</ul>
</li>

<li><b>Template mode:</b><br>
The first frame will have unassigned label points in random locations on the image (white pluses with colored numbers next to them). On subsequent frames the points will be where they were on the previous frame. To label the frame, move the pluses to where they should belong on the target.  You can drag and drop a point with the mouse or you can select (or unselect) a point with its associated number key (1-9).  When a point is selected it changes from a plus to an x.  A selected point can be moved using the arrow keys (helpful for fine-tuning positions).  Shift plus the arrow keys moves the selected point in larger jumps.  When a point is selected, a mouse click anywhere on the image will move the selected point to that location. Once you are happy with the positions of the points, click the accept button, the 's' key or the space bar to save the labels for the current frame.  

</li>


<li><b>High throughput mode:</b><br>

In HT mode, you label the entire movie for point 1, then you label the entire movie for point 2, etc. Click the image to label a point. After clicking/labeling, the movie is automatically advanced. The number of frames to advance is set in 'Set Frame Increment' in the Label dropdown menu. When the end of the movie is reached, the labeling point is incremented, until all labeling for all points is complete. You may manually change the current labeling point in 'Set Labeling Point' in the Label dropdown menu.

Right-clicking the current point offers options for repeatedly accepting the current point. Right-clicking the image labels a point as an "occluded estimate", ie the clicked location represents your best guess at an occluded landmark.

</li>


<li><b>Multiview calibrated:</b><br>

For calibrated labeling, first go to Setup> Load Calibration File to load a calibration file. This enables calibrated labeling, where epipolar lines and reconstructed points are shown to assist in labeling multi-camera data.

The number keys 0-9 select a physical point. Each view has its own projection of this point and the selected point will be indicated on all views. After a point is selected, clicking on any view will jump the point in that view to the clicked location. Epipolar lines will be projected in the other views. The point in the first/original view is now adjustable by click-dragging, or with the arrow or -arrow keys. The epipolar lines should live-update as the first point is adjusted.

For projects with three or more views: Clicking on a second view jumps the selected point in the second view to the clicked location. With the point anchored in two views, reconstructed best-guesses for the point are shown in all remaining views. The reconstruction shows three points indicating a spread of possible locations based on the first two clicked locations. The middle point is the most likely location.

Spacebar toggles the anchoring state of a point in a view. Anchored points have the letter 'a' appended to their text label. When a point is anchored, epipolar lines or reconstructed points are shown in the other views. Points that are shown as 'x' rather than '+' are adjustable with the arrow keys (fine adjustment) or -arrow keys (coarse adjustment).

When all points are in their desired locations, the Accept button (or 's' hotkey) accepts the labels.

</li>
</ul>


Once you have a reasonable number of frames labeled you are ready to train a classifier.</p>

To remove frames (useful for making a subset, or if you change your mind about how something should be labeled), you can just click the "Clear" button on that frame (next to the Accepted button), and those points will go back to unassigned.



<hr class="h2-divider">

<h2><a id="Setting the tracking parameters"> Setting the tracking parameters </a></h2>

How well APT works depends critically on getting the tracking parameters set well for the particular application.  Each tracker has different parameters.  

<p>Select 'Configure tracking parameters' from the Track dropdown menu. This will popup a <a href="images/apt_track_configure_tracking_parameters.png" target="_blank"> <b>tracking parameters window.</b></a></p>

What displays will depend on what type of tracking you are using.  Clicking on each option will produce a description of that option at the bottom of the page. Note that many of the default options are selections from dropdown menus, click the current option to see the other options. 

<p>Once you have set the parameters to what you would like, hit the Apply button.</p>

<a href="tracking_parameters.html#CPR">CPR Tracking parameters</a>

<hr class="h2-divider">

<h2><a id="Training"> Training </a></h2>

Click the Train button.


<hr class="h2-divider">

<h2><a id="Tracking"> Tracking </a></h2>

Track button
Click Track. You can change what frames you want it to track with the drop down menu below the blue "Track" button. After you choose what frames you want to track, press "Track". Watch progress in MATLAB command line. 
 
 <p>Tracking options in the Track dropdown menu:</p>
<ul>
<li>Current target, labeled frames</li>
<li>Current target, all frames</li>
<li>Current target, all frames, every 10 frames</li>
<li>Current target, selected frames</li>
<li>Current target, selected frames, every 10 frames</li>
<li>Current target, within 100 frames of current frame</li>
<li>Current target, within 100 frames of current frame, every 10 frames</li>
<li>All targets, labeled frames</li>
<li>All targets, all frames</li>
<li>All targets, all frames, every 10 frames</li>
<li>All targets, selected frames</li>
<li>All targets, selected frames, every 10 frames</li>
<li>All targets, within 100 frames of current frame</li>
<li>All targets, within 100 frames of current frame, every 10 frames</li>
</ul>

<hr class="h2-divider">

<h2><a id="Exporting tracking results"> Exporting tracking results  </a></h2>

Track->Export current tracking results->Current movie only 
will export the predicted tracks for the current movie to a trk file.

Track->Export current tracking results-> All movies
will export the predicted tracks for all the movies to trk files

<p><a href="appendices.html#Trk file contents">Contents of a trk file</a></p>

<hr class="h2-divider">

<h2><a id="Exporting manual labels"> Exporting manual labels  </a></h2>

<p>Exporting manual labels to a .trk file:</p>
Select File -> Import/Export -> Export Labels to Trk Files
The first time you do this it will save to the same directory as your movie files, with a filename of [movie file name]_[labeler project name]_labels.trk.  If you go to export again, it will prompt for overwriting, adding datetime or canceling the export.  Note that the _labels part of the filename distinguishes between a trk file of manual labels and a trk file of automatically generated labels.

<hr class="h2-divider">

<h2><a id="Evaluating performance"> Evaluating performance  </a></h2>

There are two ways to evaluate performance in APT: Cross Validation and Ground-Truthing Mode. Both of these options are found under the "Evaluate" tab in the main menu bar.

<h3><a id="Cross validation"> Cross validation  </a></h3>

APT uses k-fold cross validation, in which you train on (k-1)/k of the labels, and test on the held out 1/k labels for a partitioning of the labels into k sets. 

To start Evaluate > Cross validation

THis will pop up a window that prompts for number of k-folds.

When cross validation is done running it will pop up a window with two buttons, "Export Results to Workspace" and "View Results in APT"
<br>

Pressing "Export Results to Workspace" will pop up a window that says "Wrote variable aptXVresults in base workspace", you can then manually save that variable.  Note that to load that variable in later to evaluate it you need to run APT.setpath to set the MovieIndex class.


<p>Structure of the aptXVresults variable:
it is a [number of labeled targets] x 9 cell array, with the following columns:</p>
<ol>
<li> fold is the cross-validation set/fold index.</li>
<li> mov is the movie index (into .movieFilesAll). </li>
<li> frm is the frame. </li>
<li> iTgt is the target index (index into .trx). </li>
<li> tfocc is a [1xnpt] logical, true if pt is occluded. </li>
<li> p is the GT/labeled position vector -- all x coords, then all y coords, so should be [1x2*npts]. </li>
<li> roi is a [1x4] [xlo xhi ylo yhi] for the cropping region when there are trx. xhi-xlo and yhi-ylo are set by Track->Configure tracking parameters->Multiple Targets -> Target crop radius </li>
<li> pTrk is like p, but it is the CPR-tracked position vector. </li>
<li> dGTTrk I think is [1xnpts], euclidean distance from p to pTrk for each pt.</li>
</ol>
[This is saved into the .lbl file now, and there is also a command to delete it]

<h3><a id="Ground-Truthing mode"> Ground-Truthing mode </a></h3>

Ground-Truthing mode enables you to assess the performance of your tracker on an unbiased set of APT-generated test frames.

<p>Create a project, add movies, label frames, and train a tracker iteratively as you normally would in APT.</p>

<p>Select Evaluate>Ground-Truthing Mode. An orange "GT Mode" indicator should appear in the main UI, and the Manage Movies window should appear.</p>

<p>Manage Movies is now tabbed, with the "GT Movie List" tab selected. The project now contains two sets of movies: i) "regular" movies used for training and parameter refinement, and ii) "GT" movies for testing tracker performance.
Add test movies to the GT movie list. If possible, it is best to use movies that are not also in the regular movie list, ie that the project has never seen before.
When the movie list is complete, press the "GT Frames" button to bring up the Ground-Truthing window. The project can be saved at any time during this process. (If you close GT window it can be re-opened from movie manager GUI). </p>

<p>In the Ground-Truthing window, press the Suggest button to generate a new/fresh list of frames to label. At the moment, frames are sampled randomly from the available GT movies, with all frames equally weighted. Other options are available at the command-line (see below).
Click on a row in the table to navigate to a frame, or use the "Next Unlabeled" button. The APT main axes should become highlighted, indicating that the current frame/target is a GT frame. Label this frame. These labels will be used as GT labels against which the tracker will be compared.</p>

<p>When all GT frames are labeled, press "Compute GT Performance". APT will track the GT frames using the trained tracker, and compare the results to the manual GT labels.
Along with various plots, the Labeler property .gtTblRes provides a results table for each GT row: manual labels, tracked positions, and L2 error.</p>

<p>Save the project to preserve your GT movie list, the list of GT frames with their labels, and the GT results.</p>




<footer>
<hr class="h1-divider">
<center>
<a href="index.html">APT Documentation Home</a> | <a href="https://www.janelia.org/lab/branson-lab">Branson Lab</a> | <i>Last Updated June 5, 2018</i>
</center>
</footer>


</body>
