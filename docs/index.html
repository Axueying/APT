<!DOCTYPE html>
<html>
<head>
<title>APT Documentation</title>
<link rel="stylesheet" type="text/css" charset="utf-8" media="all" 
href="styles/common.css">
<link rel="stylesheet" type="text/css" charset="utf-8" media="screen" 
href="styles/screen.css">
<link rel="stylesheet" type="text/css" charset="utf-8" media="print" 
href="styles/print.css">
<link rel="stylesheet" type="text/css" charset="utf-8" 
media="projection" href="styles/projection.css">

<style type="text/css">
strong.regular-font {
  font-family: Arial, Lucida Grande, sans-serif;
  font-style: italic;
  font-size: 0.9em;
}
</style>

</head>

<body>
<h1><a href="index.html">APT</a> - The Animal Part Tracker</h1>

<br>
<br>
<center><img style='border:2px solid #000000' src="images/apt_examples.png" width="50%"></center>
<br/>

<p>The <b>Animal Part Tracker</b> (<b>APT</b>) is machine learning software for automatically tracking the locations of body parts in input videos. For example, it has been used to track the tips of the legs of a fly or the ears of a mouse. To train a tracker, a user labels the locations of selected landmark points in selected frames and videos. Machine learning is then used to train a classifier which can automatically predict the locations of these parts in new frames and videos.</p>

<p>APT is modular software that has been engineered to work efficiently for a variety of applications. It has been used to track parts of flies, mice, and larvae. It can track parts in two dimensions from video from a single camera, or in three dimensions from video from multiple calibrated cameras. It can also be used to track the parts of multiple interacting animals. APT includes both an engineered labeling interface as well as machine learning algorithms for training trackers.</p>

<p>APT is developed by Allen Lee, Mayank Kabra, Alice Robie, Stephen Huston, Felipe Rodriguez, Roian Egnor, Austin Edwards, and Kristin Branson. All work is funded by the Howard Hughes Medical Institute and the Janelia Research Campus.</p>

<p>This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the <a href="GNU_GPL_v3.html">GNU General Public License (version 3)</a> for more details.</p>

<hr class="h2-divider">

<h2>Links</h2>
<ul>
<li> <b>Git repository</b> with the latest version of the code:
<a href="https://github.com/kristinbranson/APT">https://github.com/kristinbranson/APT</a></li>
<li> <b>Branson lab</b> at HHMI Janelia Research Campus: <a href="http://www.janelia.org/lab/branson-lab">http://www.janelia.org/lab/branson-lab</a></li>
<li> <b>Contact information</b>: Kristin Branson bransonk@janelia.hhmi.org</li>
</ul>

<hr class="h2-divider">

<h2>Contents</h2>

<ul>
<li><a href="#Download">Download</a></li>
<li><a href="#Requirements">Requirements</a></li>
<!--
<li><a href="#Install">Installation</a></li>
-->
<li><a href="#Startup">Start-up</a></li>
<li><a href="#Setting up a new project">Setting up a new project</a></li>
<li><a href="#Opening an existing project">Opening an existing project</a></li>
<li><a href="#Navigating in APT">Navigating in APT</a></li>
<li><a href="#Labeling points">Labeling points</a></li>
<li><a href="#Setting the tracking parameters">Setting the tracking parameters</a></li>
<li><a href="#Training">Training</a></li>
<li><a href="#Tracking">Tracking</a></li>
<li><a href="#Exporting tracking results">Exporting tracking results</a></li>
<li><a href="#Exporting manual labels">Exporting manual labels</a></li>
<li><a href="#Cross validation">Cross validation</a></li>
<li><a href="#Ground-Truthing mode">Ground-Truthing mode</a></li>
<li><a href="appendices.html">Appendices</a></li>

</ul>

<hr class="h2-divider">

<h2><a id="Download">Download</a></h2>

<p>The <a href="https://github.com/kristinbranson/APT">git repository</a> for APT is hosted on <a href="https://github.com/kristinbranson/APT"><b>Github</b></a>. This software is currently under heavy development, so we recommend using git to <a href="https://help.github.com/en/articles/cloning-a-repository">clone</a> the repository and updating frequently.</p>

<!--
<p>APT requires the following other code packs; please also download these:
<ul>
<li> <a href="https://github.com/kristinbranson/JAABA">JAABA</a>: The Janelia Automatic Animal Behavior Annotator.</li>
<li> <a href="https://github.com/pdollar/toolbox">Piotr Dollar's Computer Vision Toolbox</a>.
</ul>
</p>
-->

<!-- <p>If you are working with multiple calibrated cameras, you may need calibration software for your rig, e.g <a href="https://www.vision.caltech.edu/bouguetj/calib_doc/"> Caltech camera calibration toolbox </a>
-->

<hr class="h2-divider"/>

<h2><a id="Requirements">Requirements</a></h2>

<h3>MATLAB</h3>

<p>APT requires <a href="https://www.mathworks.com/products/matlab.html">MATLAB</a> version >= 2017a and the following MATLAB toolboxes:
<ul>
<li>Image Processing Toolbox</li>
<li>Parallel Computing Toolbox</li>
<li>Statistics and Machine Learning Toolbox</li>
<li>Curve Fitting Toolbox (<i>Multi-camera only</i>)</li>
<li>Computer Vision System Toolbox (<i>Multi-camera only</i>)</li>
<li>Optimization Toolbox (<i>Multi-camera only</i>)</li>
</ul>
</p>

<h3>GPU</h3>

<p>A <a href="https://lambdalabs.com/blog/best-gpu-tensorflow-2080-ti-vs-v100-vs-titan-v-vs-1080-ti-benchmark">GPU</a> is required to train a deep-learning based tracker. APT can use any of:
<ul>
<li>GPUs on your local machine via Docker.</li>
<li>GPUs on the Amazon Cloud (AWS).</li>
<li>The Janelia GPU cluster.</li>
</ul>
We refer to this as the <a href="#BackEnd"><b>back end</b></a>, and more information about setting up each of these back ends is <a href="#BackEnd">here</a>. 

<!--
<hr class="h2-divider">

<h2><a id="Install">Installation</a></h2>

In the APT directory, copy Manifest.sample.txt  to Manifest.txt and edit to point to your copy of JAABA and Piotr's toolbox (specify the root directory, which contains the subfolders filehandling/ and misc/, 
e.g. jaaba, c:\pgms\jaaba
	piotr, c:\pgms\pdollar_toolbox).
 
NOTE: In Manifest.sample.txt there is also a path to camera/calibration/toolbox, if you are not using multi-camera it is OK to not include this line.
-->


<hr class="h2-divider">

<h2><a id="Startup">Start-up</a></h2>

APT is a MATLAB-based program. To run it:
<ol>
<li> Start MATLAB. </li>
<li> Within MATLAB, change into the directory containing the APT code:
<pre>>> cd APT;</pre>
</li>

<li> Run <b>StartAPT</b> on the command line to start APT:
<pre>>> lObj = StartAPT; 
</pre>
</li>
The <a href="images/apt_open_screen.png" target="_blank">main APT GUI</a> will then appear. 
</ol>

<hr class="h2-divider">

<h2><a id="BackEnd">GPU Back ends</a></h2>

To train and track using deep learning, APT can either use GPUs on your local machine, or remote GPUs in the Amazon Cloud (AWS) or on Janelia's GPU cluster (Janelians-only). 

<hr class="h2-divider">

<h2><a id="Setting up a new project"> Setting up a new project </a></h2>
From the File dropdown menu, select "New Project...".  This will pop up a <a href="images/apt_new_project_dialog.png" target="_blank"> New Project dialog. </a>  Enter a project name, the number of body points to detect, the number of views, and check the "body tracking" box if your animal(s) are tracked. Click "Create Project".  This will open a <a href="images/apt_new_project_add_movies.png" target="_blank">Manage Movies dialog.</a>  Click "Add Movie" and select the first movie to add.  If you selected body tracking, you will also be prompted to select the tracking file associated with this movie.  You may add additional movies at any time.  The Manage Movies dialog box can be used to add more movies and also navigate between movies during labeling. Close or minimize the Manage Movie window. Select "Save as ..." from the File dropdown menu and save your project. This will save a .lbl file with your project in it.

<hr class="h2-divider">

<h2><a id="Opening an existing project"> Opening an existing project </a></h2>

Use "Load Project" from the File dropdown menu and select the .lbl file to load. If the locations of the movies in the project has changed since you last opened the project it will prompt for their new locations.  


<hr class="h2-divider">

<h2><a id="Navigating in APT"> Navigating in APT </a></h2>

The  <a href="images/apt_gui_layout_develop_labeled.png" target="_blank"> <b> GUI is divided into several sections.</b> </a>  Hovering over different components will reveal tooltips.

To navigate through the video in the Labeling window:
a/d, -/=, or the left and right arrow keys all move one frame left/right respectively. Using these keys while holding down the ctrl button uses 10 frames steps (this default step size can be changed in the Go dropdown menu, under "Navigation preferences..."). You can also navigate using the mouse and the navigation bar under the image.  Clicking on the bar to the left or right of the slider jumps by 100 frames.  Single clicks on the arrows at the ends of the navigation bar moves by a single frame.  Holding the end arrows down steps through the movie a frame at a time. To look at only labeled frames, hold the shift button while using the right and left arrow keys.
<br>
<br>
Useful view options for multi-target projects with tracking and orientation:<br>
<i>From the View dropdown menu:</i><br>
Center video on target  - keeps current target in the center of the view<br>
Rotate video so target always points up - does just that<br>
Zoom out/full images or unzoom button - view full image <br>
<br>
In the Target table window: <br>
Use slider bar above ‘unzoom’ to adjust zoom, not the magnifier +/- <br>
Zoom slider bar - adjust zoom then ‘set’, ‘recall’ will change back to ‘set’ value after changing movies. 

<p>To just look at labeled frames, holding the shift button while using the right and left arrow keys will move through only the labeled frames, skipping the unlabeled ones. </p>


<hr class="h2-divider">

<h2><a id="Labeling points"> Labeling points </a></h2>

Labeling modes:
There are different labeling modes, which you can switch between in the Label dropdown menu.

<ul>
<li><b>Sequential mode:</b><br>
Allows you to just click your points in order; points are automatically accepted once you click the last point.  You can edit placement after you finish by:
<ul>
<li>Clicking and dragging a point</li>
<li>Activating a point with a number key that corresponds to the label number and then clicking the correct location for the label. Activated points change from '+' to 'x' (Numbers above 10 are accessed by `(back quote) + number key).</li>
<li>Activating a point with the number keys and moving with arrow keys. Hitting the number key again will toggle the point back to '+'. If you hold down the shift key while you click, the point will be recorded as an 'o' instead of a '+', indicating an occluded point.</li> 
</ul>
</li>

<li><b>Template mode:</b><br>
The first frame will have unassigned label points in random locations on the image (white pluses with colored numbers next to them). On subsequent frames the points will be where they were on the previous frame. To label the frame, move the pluses to where they should belong on the target.  You can drag and drop a point with the mouse or you can select (or unselect) a point with its associated number key (1-9).  When a point is selected it changes from a plus to an x.  A selected point can be moved using the arrow keys (helpful for fine-tuning positions).  Shift plus the arrow keys moves the selected point in larger jumps.  When a point is selected, a mouse click anywhere on the image will move the selected point to that location. Once you are happy with the positions of the points, click the accept button, the 's' key or the space bar to save the labels for the current frame.  

</li>


<li><b>High throughput mode:</b><br>

In HT mode, you label the entire movie for point 1, then you label the entire movie for point 2, etc. Click the image to label a point. After clicking/labeling, the movie is automatically advanced. The number of frames to advance is set in 'Set Frame Increment' in the Label dropdown menu. When the end of the movie is reached, the labeling point is incremented, until all labeling for all points is complete. You may manually change the current labeling point in 'Set Labeling Point' in the Label dropdown menu.

Right-clicking the current point offers options for repeatedly accepting the current point. Right-clicking the image labels a point as an "occluded estimate", ie the clicked location represents your best guess at an occluded landmark.

</li>


<li><b>Multiview calibrated:</b><br>

For calibrated labeling, first go to Setup> Load Calibration File to load a calibration file. This enables calibrated labeling, where epipolar lines and reconstructed points are shown to assist in labeling multi-camera data.

The number keys 0-9 select a physical point. Each view has its own projection of this point and the selected point will be indicated on all views. After a point is selected, clicking on any view will jump the point in that view to the clicked location. Epipolar lines will be projected in the other views. The point in the first/original view is now adjustable by click-dragging, or with the arrow or -arrow keys. The epipolar lines should live-update as the first point is adjusted.

For projects with three or more views: Clicking on a second view jumps the selected point in the second view to the clicked location. With the point anchored in two views, reconstructed best-guesses for the point are shown in all remaining views. The reconstruction shows three points indicating a spread of possible locations based on the first two clicked locations. The middle point is the most likely location.

Spacebar toggles the anchoring state of a point in a view. Anchored points have the letter 'a' appended to their text label. When a point is anchored, epipolar lines or reconstructed points are shown in the other views. Points that are shown as 'x' rather than '+' are adjustable with the arrow keys (fine adjustment) or -arrow keys (coarse adjustment).

When all points are in their desired locations, the Accept button (or 's' hotkey) accepts the labels.

</li>
</ul>


Once you have a reasonable number of frames labeled you are ready to train a classifier.</p>

To remove frames (useful for making a subset, or if you change your mind about how something should be labeled), you can just click the "Clear" button on that frame (next to the Accepted button), and those points will go back to unassigned.



<hr class="h2-divider">

<h2><a id="Setting the tracking parameters"> Setting the tracking parameters </a></h2>

How well APT works depends critically on getting the tracking parameters set well for the particular application.  Each tracker has different parameters.  

<p>Select 'Configure tracking parameters' from the Track dropdown menu. This will popup a <a href="images/apt_track_configure_tracking_parameters.png" target="_blank"> <b>tracking parameters window.</b></a></p>

What displays will depend on what type of tracking you are using.  Clicking on each option will produce a description of that option at the bottom of the page. Note that many of the default options are selections from dropdown menus, click the current option to see the other options. 

<p>Once you have set the parameters to what you would like, hit the Apply button.</p>

<a href="tracking_parameters.html#CPR">CPR Tracking parameters</a>

<hr class="h2-divider">

<h2><a id="Training"> Training </a></h2>

Click the Train button.


<hr class="h2-divider">

<h2><a id="Tracking"> Tracking </a></h2>

Track button
Click Track. You can change what frames you want it to track with the drop down menu below the blue "Track" button. After you choose what frames you want to track, press "Track". Watch progress in MATLAB command line. 
 
 <p>Tracking options in the Track dropdown menu:</p>
<ul>
<li>Current target, labeled frames</li>
<li>Current target, all frames</li>
<li>Current target, all frames, every 10 frames</li>
<li>Current target, selected frames</li>
<li>Current target, selected frames, every 10 frames</li>
<li>Current target, within 100 frames of current frame</li>
<li>Current target, within 100 frames of current frame, every 10 frames</li>
<li>All targets, labeled frames</li>
<li>All targets, all frames</li>
<li>All targets, all frames, every 10 frames</li>
<li>All targets, selected frames</li>
<li>All targets, selected frames, every 10 frames</li>
<li>All targets, within 100 frames of current frame</li>
<li>All targets, within 100 frames of current frame, every 10 frames</li>
</ul>

<hr class="h2-divider">

<h2><a id="Exporting tracking results"> Exporting tracking results  </a></h2>

Track->Export current tracking results->Current movie only 
will export the predicted tracks for the current movie to a trk file.

Track->Export current tracking results-> All movies
will export the predicted tracks for all the movies to trk files

<p><a href="appendices.html#Trk file contents">Contents of a trk file</a></p>

<hr class="h2-divider">

<h2><a id="Exporting manual labels"> Exporting manual labels  </a></h2>

<p>Exporting manual labels to a .trk file:</p>
Select File -> Import/Export -> Export Labels to Trk Files
The first time you do this it will save to the same directory as your movie files, with a filename of [movie file name]_[labeler project name]_labels.trk.  If you go to export again, it will prompt for overwriting, adding datetime or canceling the export.  Note that the _labels part of the filename distinguishes between a trk file of manual labels and a trk file of automatically generated labels.

<hr class="h2-divider">

<h2><a id="Evaluating performance"> Evaluating performance  </a></h2>

There are two ways to evaluate performance in APT: Cross Validation and Ground-Truthing Mode. Both of these options are found under the "Evaluate" tab in the main menu bar.

<h3><a id="Cross validation"> Cross validation  </a></h3>

APT uses k-fold cross validation, in which you train on (k-1)/k of the labels, and test on the held out 1/k labels for a partitioning of the labels into k sets. 

To start Evaluate > Cross validation

THis will pop up a window that prompts for number of k-folds.

When cross validation is done running it will pop up a window with two buttons, "Export Results to Workspace" and "View Results in APT"
<br>

Pressing "Export Results to Workspace" will pop up a window that says "Wrote variable aptXVresults in base workspace", you can then manually save that variable.  Note that to load that variable in later to evaluate it you need to run APT.setpath to set the MovieIndex class.


<p>Structure of the aptXVresults variable:
it is a [number of labeled targets] x 9 cell array, with the following columns:</p>
<ol>
<li> fold is the cross-validation set/fold index.</li>
<li> mov is the movie index (into .movieFilesAll). </li>
<li> frm is the frame. </li>
<li> iTgt is the target index (index into .trx). </li>
<li> tfocc is a [1xnpt] logical, true if pt is occluded. </li>
<li> p is the GT/labeled position vector -- all x coords, then all y coords, so should be [1x2*npts]. </li>
<li> roi is a [1x4] [xlo xhi ylo yhi] for the cropping region when there are trx. xhi-xlo and yhi-ylo are set by Track->Configure tracking parameters->Multiple Targets -> Target crop radius </li>
<li> pTrk is like p, but it is the CPR-tracked position vector. </li>
<li> dGTTrk I think is [1xnpts], euclidean distance from p to pTrk for each pt.</li>
</ol>
[This is saved into the .lbl file now, and there is also a command to delete it]

<h3><a id="Ground-Truthing mode"> Ground-Truthing mode </a></h3>

Ground-Truthing mode enables you to assess the performance of your tracker on an unbiased set of APT-generated test frames.

<p>Create a project, add movies, label frames, and train a tracker iteratively as you normally would in APT.</p>

<p>Select Evaluate>Ground-Truthing Mode. An orange "GT Mode" indicator should appear in the main UI, and the Manage Movies window should appear.</p>

<p>Manage Movies is now tabbed, with the "GT Movie List" tab selected. The project now contains two sets of movies: i) "regular" movies used for training and parameter refinement, and ii) "GT" movies for testing tracker performance.
Add test movies to the GT movie list. If possible, it is best to use movies that are not also in the regular movie list, ie that the project has never seen before.
When the movie list is complete, press the "GT Frames" button to bring up the Ground-Truthing window. The project can be saved at any time during this process. (If you close GT window it can be re-opened from movie manager GUI). </p>

<p>In the Ground-Truthing window, press the Suggest button to generate a new/fresh list of frames to label. At the moment, frames are sampled randomly from the available GT movies, with all frames equally weighted. Other options are available at the command-line (see below).
Click on a row in the table to navigate to a frame, or use the "Next Unlabeled" button. The APT main axes should become highlighted, indicating that the current frame/target is a GT frame. Label this frame. These labels will be used as GT labels against which the tracker will be compared.</p>

<p>When all GT frames are labeled, press "Compute GT Performance". APT will track the GT frames using the trained tracker, and compare the results to the manual GT labels.
Along with various plots, the Labeler property .gtTblRes provides a results table for each GT row: manual labels, tracked positions, and L2 error.</p>

<p>Save the project to preserve your GT movie list, the list of GT frames with their labels, and the GT results.</p>




<footer>
<hr class="h1-divider">
<center>
<a href="index.html">APT Documentation Home</a> | <a href="https://www.janelia.org/lab/branson-lab">Branson Lab</a> | <i>Last Updated June 5, 2018</i>
</center>
</footer>


</body>
